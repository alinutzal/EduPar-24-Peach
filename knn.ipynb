{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor Benchmark Assignment\n",
    "\n",
    "Performing an exhaustive exact k-nearest neighbor (kNN) search, commonly referred to as a brute-force search, is computationally expensive and does not scale efficiently with larger datasets. In vector search, this method involves calculating the distance between each query vector and every vector in the database. For widely used metrics like Euclidean and cosine distances, this process essentially transforms into a large-scale matrix multiplication problem.\n",
    "\n",
    "In the time of AI, vector search or similarity search is becoming one of the hottest topics because of its application in large language models (LLM) and generative AI. In this context similarity search can be applied in many domains from detecting fraudulent transactions, recommending products to users, retrieving faces or connecting space points.\n",
    "In this assignment we are exploring different method for performing knn search with the possibility of running these methods on the GPU to speed it up.\n",
    "\n",
    "First, we need to create a conda environment with all the necessary packages.\n",
    "\n",
    "In the knn notebook we show 4 functions for computing the k nearest neighbors: sklearn, cuml, raft and frnn. SkLearn runs on the CPU while the other three run on the GPU. First we are randomly generating a dataset and a set of queries on. The %%timeit decorator is used to run each cell 1000 times and record the time it takes to run each loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cupy as cp\n",
    "from pylibraft.common import DeviceResources\n",
    "from sklearn.neighbors import NearestNeighbors as skNearestNeighbors\n",
    "from cuml.neighbors import NearestNeighbors as cuNearestNeighbors\n",
    "from pylibraft.neighbors.brute_force import knn\n",
    "import frnn\n",
    "import h5py\n",
    "import urllib\n",
    "\n",
    "\n",
    "n_samples = 10000\n",
    "n_features = 25\n",
    "n_queries = 1000\n",
    "rad = 0.4\n",
    "dataset = cp.random.random_sample((n_samples, n_features),\n",
    "                                  dtype=cp.float32)\n",
    "# Search using the built index\n",
    "queries = cp.random.random_sample((n_queries, n_features),\n",
    "                                  dtype=cp.float32)\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run the following 4 cells and record the timing it takes to run each loop of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k Nearest Neighbor from Sklearn (CPU)\n",
    "%%timeit\n",
    "neighbors = skNearestNeighbors(n_neighbors=k)\n",
    "neighbors.fit(cp.asnumpy(dataset))\n",
    "for el in cp.asnumpy(queries):\n",
    "    neighbors.kneighbors([el])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k Nearest Neighbor from Rapids AI cuML (GPU)\n",
    "%%timeit\n",
    "cu_neighbors = cuNearestNeighbors(n_neighbors=k)\n",
    "cu_neighbors.fit(dataset)\n",
    "cu_neighbors.kneighbors(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k Nearest Neighbor brute force from Rapids AI raft (GPU)\n",
    "%%timeit\n",
    "distances, neighbors = knn(dataset, queries, k)\n",
    "distances = cp.asarray(distances)\n",
    "neighbors = cp.asarray(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radius Nearest Neighbor for (GPU)\n",
    "%%timeit\n",
    "dists, idxs, _, _ = frnn.frnn_grid_points(\n",
    "    points1=cp.asnumpy(queries),\n",
    "    points2=cp.asnumpy(dataset),\n",
    "    lengths1=None,\n",
    "    lengths2=None,\n",
    "    K=k,\n",
    "    r=rad,\n",
    "    grid=None,\n",
    "    return_nn=False,\n",
    "    return_sorted=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot the times using a barplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Increase the size of the dataset using the following values: 20,000 50,000 and 100,000. For each value run the 4 knn methods and record the timing. Plot them together with the initial experiments for dataset of size 10,000 and make a summary plot.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Only the SkLearn and frnn can perform a fixed radius search, that doesn't require sorting the result list to make the cut cording with the number of neighbors k. This means that for the fixed radius search sorting is not necessary and it can be performed much faster. Run fixed radius search using these two functions and compare them using a bar plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load the dataset sift-128-euclidean from the ann-benchmarks and repeat the same experiments on the dataset using k = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_url=\"http://ann-benchmarks.com/sift-128-euclidean.hdf5\", work_folder=None):\n",
    "    \"\"\"Download dataset from url. It is expected that the dataset contains a hdf5 file in ann-benchmarks format\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "      dataset_url address of hdf5 file\n",
    "      work_folder name of the local folder to store the dataset\n",
    "\n",
    "    \"\"\"\n",
    "    dataset_filename = dataset_url.split(\"/\")[-1]\n",
    "\n",
    "    # We'll need to load store some data in this tutorial\n",
    "    if work_folder is None:\n",
    "        work_folder = os.path.join(tempfile.gettempdir(), \"raft_example\")\n",
    "\n",
    "    if not os.path.exists(work_folder):\n",
    "        os.makedirs(work_folder)\n",
    "    print(\"The index and data will be saved in\", work_folder)\n",
    "\n",
    "    ## download the dataset\n",
    "    dataset_path = os.path.join(work_folder, dataset_filename)\n",
    "    if not os.path.exists(dataset_path):\n",
    "        urllib.request.urlretrieve(dataset_url, dataset_path)\n",
    "\n",
    "    f = h5py.File(dataset_path, \"r\")\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_FOLDER = os.path.join(tempfile.gettempdir(), \"raft_example\")\n",
    "f = load_dataset(\"http://ann-benchmarks.com/sift-128-euclidean.hdf5\", work_folder=WORK_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 (Extra credit) Implement any approximate search from raft such as CAGRA, HNSW, IVF-FLAT, or IVF-PQ. Are these method providing any speed up over the brut force implementation from raft?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acorn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
